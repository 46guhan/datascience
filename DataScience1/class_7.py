#Naive Bayes
"""
Part 1: What is Naive Bayes?
Naive Bayes is a probabilistic machine learning algorithm based on Bayesâ€™ Theorem. Itâ€™s called "naive" because it assumes that all features are independent of each other (which is rarely true in reality but often works surprisingly well).


 
P(Y|X) â†’ Posterior: Probability of class Y given features X

P(X|Y) â†’ Likelihood: Probability of features X given class Y

P(Y) â†’ Prior: Probability of class Y

P(X) â†’ Evidence: Probability of features X

ğŸ“Œ Intuition
Imagine you're trying to classify whether an email is spam or not based on keywords. Naive Bayes calculates the probability that the email is spam given the presence (or absence) of words, assuming each word is independent.

ğŸ”— Types of Naive Bayes:
Type	When to Use	Works With
GaussianNB	When features are continuous/numeric	Normal distribution (e.g., age, salary)
MultinomialNB	When features are discrete counts	Word counts in text (e.g., spam detection)
BernoulliNB	When features are binary	Yes/No, 0/1 data (e.g., click/no click)
ğŸ“Š Part 2: What Kind of Data Can You Use?
Type of Feature	Use with Naive Bayes?	Notes
Categorical (Text)	âœ… Yes (after encoding)	Use LabelEncoding or OneHotEncoding
Numerical	âœ… Yes	Especially with GaussianNB
Binary (0/1)	âœ… Yes	Works great with BernoulliNB
Text Data	âœ… Yes	Use MultinomialNB after converting to bag-of-words or TF-IDF
ğŸ‘‡ Example Features for Employee Dataset (With GaussianNB):
Feature	Type	Use with NB
Age	Numeric	âœ… Yes (GaussianNB)
Department	Categorical	âœ… Yes (after encoding)
YearsAtCompany	Numeric	âœ… Yes
JobSatisfaction	Ordinal	âœ… Yes
Attrition (Label)	Binary	âœ… Yes (target)
âœ… Summary
Aspect	Naive Bayes
ğŸ“š Type	Classifier (Probabilistic)
ğŸ§  Assumption	Features are conditionally independent
ğŸ’¡ Good For	Text classification, simple datasets
ğŸ“ˆ Input Type	Numeric, categorical (after encoding), binary
â±ï¸ Speed	Very fast, low memory
ğŸš« Not Good For	Highly correlated features, complex relationships

"""
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score


df=pd.read_csv("dataset/employee_supervised_dataset.csv")

df_encode=df.copy()
le=LabelEncoder()
df_encode["Department"]=le.fit_transform(df_encode["Department"])
df_encode["Attrition"]=le.fit_transform(df_encode["Attrition"])


X=df_encode.drop(["EmployeeID","Attrition"],axis=1)
y=df_encode["Attrition"]

# X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)
# model=GaussianNB()
# model.fit(X_train,y_train)

# y_pred=model.predict(X_test)

# accuracy=accuracy_score(y_test,y_pred)

# print(accuracy)

model=GaussianNB()
model.fit(X,y)

ypred=model.predict(X)
print(ypred)
accuracy=accuracy_score(y,ypred)
print(accuracy)